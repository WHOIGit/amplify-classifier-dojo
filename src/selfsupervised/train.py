import os
import argparse
import random

import coolname
import torchvision
from torchvision.transforms import v2
from aim.pytorch_lightning import AimLogger
from dotenv import load_dotenv

import torch
import lightning.pytorch as pl
from lightning.pytorch.callbacks import ModelCheckpoint
from lightning.pytorch.tuner import Tuner

# set import paths to project root
if __name__ == '__main__':
    import sys, pathlib
    PROJECT_ROOT = pathlib.Path(__file__).parent.parent.parent.absolute()
    if sys.path[0] != str(PROJECT_ROOT): sys.path.insert(0, str(PROJECT_ROOT))

from src.multiclass.models import check_model_name, get_model_base_transforms, get_namebrand_model, get_model_resize
from src.selfsupervised.datasets import IfcbDatamodule
from src.selfsupervised.models import SimCLR
from src.multiclass.callbacks import LogNormalizedLoss
from src.train import setup_aimlogger


def argparse_init(parser=None):
    if parser is None:
        parser = argparse.ArgumentParser(description='Train an image classifier!')

    # DATASET #
    dataset = parser.add_argument_group(title='Dataset', description=None)
    dataset.add_argument('--trainlist', required=True, help='A text file, one ifcb bin-directory per line.')
    dataset.add_argument('--classlist', help='A text file, each line is a class label (the label order is significant)')
    dataset.add_argument('--vallist', help='A text file, one sample per line, each sample has a class-index and image path')
    dataset.add_argument('--knnlist', help='Embeddings VALLIST samples will be k-nearest-neighbored with')
    dataset.add_argument('--knn-k', metavar='K', type=int, default=11, help='Number K of nearest-neighbors to check. Default is 11')
    dataset.add_argument('--shuffle-buffer', type=int, default=1000, help='How many ROIs to gather before shuffling. 1 to disable buffer, 0 to disable shuffling. Default is 1000 ')

    # TRACKING #
    aimstack = parser.add_argument_group(title='AimLogger', description=None)
    aimstack.add_argument('--run', help='The name of this run. A run name is automatically generated by default')
    aimstack.add_argument('--experiment', help='The broader category/grouping this RUN belongs to')
    aimstack.add_argument('--note', help='Add any kind of note or description to the trained model. Make sure to use quotes "around your message."')
    aimstack.add_argument('--repo', help='Aim repo path. Also see: Aim environment variables.')
    aimstack.add_argument('--artifacts-location', help='Aim Artifacts location. Also see: Aim environment variables.')
    #aimstack.add_argument('--plot', nargs='+', action='append', ...)
    #aimstack.add_argument('--callback', nargs='+', action='append', ...)
    #aimstack.add_argument('--metric', nargs='+', action='append', ...)

    # HYPER PARAMETERS #
    model = parser.add_argument_group(title='Model Parameters')
    model.add_argument('--model-name', help='Model Class/Module Name or torch model checkpoint file', required=True)  # TODO checkopint file, also check loading from s3
    model.add_argument('--weights', default='DEFAULT', help='''Specify a model's weights. Either "DEFAULT", some specific identifier, or "None" for no-pretrained-weights''')
    model.add_argument('--seed', type=int, help='Set a specific seed for deterministic output')
    model.add_argument('--batch', dest='batch_size', metavar='SIZE', default=256, type=int, help='Number of images per batch. Defaults is 256')

    epochs = parser.add_argument_group(title='Epoch Parameters')
    epochs.add_argument('--epoch-max', metavar='MAX', default=100, type=int, help='Maximum number of training epochs. Default is 100')
    epochs.add_argument('--epoch-min', metavar='MIN', default=10, type=int, help='Minimum number of training epochs. Default is 10')
    epochs.add_argument('--epoch-stop', metavar='STOP', default=10, type=int, help='Early Stopping: Number of epochs following a best-epoch after-which to stop training. Set STOP=0 to disable. Default is 10')

    # UTILITIES #
    parser.add_argument('--checkpoints-path', default='/tmp/classifier_checkpoints')
    parser.add_argument('--autobatch', nargs='?', default=False, const='power', choices=['power','binsearch'], help='Auto-Tunes batch_size prior to training/inference.')
    parser.add_argument('--autobatch-max', type=int, help='Disallow autobatch for setting ')
    parser.add_argument('--workers', dest='num_workers', metavar='N', type=int, help='Total number of dataloader worker threads. If set, overrides --workers-per-gpu')
    parser.add_argument('--workers_per_gpu', metavar='N', default=4, type=int, help='Number of data-loading threads per GPU. 4 per GPU is typical. Default is 4')
    parser.add_argument('--fast-dev-run', default=False, action='store_true')
    parser.add_argument('--env', metavar='FILE', nargs='?', const=True, help='Environment Variables file. If set but not specified, attempts to find a parent .env file')
    parser.add_argument('--gpus', nargs='+', type=int, help=argparse.SUPPRESS) # CUDA_VISIBLE_DEVICES

    return parser


def argparse_runtime_args(args):
    # Record GPUs
    if not args.gpus:
        args.gpus = [int(gpu) for gpu in os.environ.get('CUDA_VISIBLE_DEVICES','UNSET').split(',') if gpu not in ['','UNSET']]

    if args.env:
        load_dotenv(override=True) if args.env is True else load_dotenv(args.env, override=True)
    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str,args.gpus))  # reset if not included in .env

    if not args.num_workers:
        args.num_workers = len(args.gpus)*args.workers_per_gpu

    # Record Version
    try:
        with open('VERSION') as f:
            args.version = f.read().strip()
    except FileNotFoundError:
        args.version = None

    # Set Seed. If args.seed is 0 ie None, a random seed value is used and stored
    if args.seed is None:
        args.seed = random.randint(1,2**32-1)
    args.seed = pl.seed_everything(args.seed)

    if not args.run:
        args.run = coolname.generate_slug(2)
        print(f'RUN: {args.run}')

    if args.artifacts_location and os.path.isdir(args.artifacts_location):
        args.artifacts_location = f'file://{os.path.abspath(args.artifacts_location)}'
    if 'AIM_ARTIFACTS_URI' in os.environ and os.environ['AIM_ARTIFACTS_URI']:
        if os.path.isdir(os.environ['AIM_ARTIFACTS_URI']):
            os.environ['AIM_ARTIFACTS_URI'] = f'file://{os.path.abspath(os.environ["AIM_ARTIFACTS_URI"])}'

    if args.vallist or args.knnlist or args.classlist:
        if not (args.vallist and args.knnlist and args.classlist):
            raise ValueError('Incomplete Mutually-Inclusive args: --vallist --knnlist --classlist')

def setup_model_and_datamodule(args):

    # Model-dependant Dataset Params
    assert args.model_name=='resnet18'
    args.model = check_model_name(args.model_name)
    resize = get_model_resize(args.model_name)
    from lightly.transforms import SimCLRTransform
    transform = SimCLRTransform(input_size=resize, vf_prob=0.5, hf_prob=0.5, cj_prob=0.5, cj_strength=0.5)
    eval_transform = None
    if args.classlist:
        eval_transforms = get_model_base_transforms(args.model_name)
        eval_transform = v2.Compose(eval_transforms)

    # Datamodule
    datamodule = IfcbDatamodule(args.trainlist, transform,
                                args.knnlist, args.vallist,
                                args.classlist, eval_transform,
                                batch_size=args.batch_size, num_workers=args.num_workers,
                                shuffler_buffer_size=args.shuffle_buffer, use_len=True)
    knn_dataloader = datamodule.knn_dataloader()if args.knnlist else []

    # todo make a backbone creator
    #get_namebrand_model
    #backbone_model, output_feature_num = get_namebrand_model_backbone(model_name, weights)
    # using a resnet backbone
    resnet = torchvision.models.resnet18()
    backbone = torch.nn.Sequential(*list(resnet.children())[:-1])
    backbone_outfeatures = resnet.fc.in_features

    # TODO other architectures, like VICReg
    model = SimCLR(backbone, backbone_outfeatures, 128, knn_dataloader, knn_k=args.knn_k)  # TODO what is mystery number 126
    return model, datamodule



def main(args):
    torch.set_float32_matmul_precision('medium')

    ## Setup Model & Data Module ##
    model, datamodule = setup_model_and_datamodule(args)

    ## Setup Epoch Logger ##
    # val_/train_ already handled by default
    contexts = dict(averaging={'macro': '_macro', 'micro': '_micro', 'weighted': '_weighted',
                               'none': '_perclass'},  # f1, precision, recall
                    )
    logger = setup_aimlogger(args, contexts)

    ## Setup Callbacks ##
    callbacks=[]

    validation_results_callbacks = [
        LogNormalizedLoss(),
    ]
    callbacks.extend(validation_results_callbacks)

    # Checkpointing
    # https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.ModelCheckpoint.html
    # https://lightning.ai/docs/pytorch/stable/common/checkpointing_advanced.html
    hashid = logger.experiment.hash if isinstance(logger,AimLogger) else logger[0].experiment.hash
    chkpt_path = os.path.join(args.checkpoints_path,hashid)
    ckpt_callback = ModelCheckpoint(
        dirpath=chkpt_path, filename='best.ckpt',
        monitor='val_loss', mode='min')
    callbacks.append(ckpt_callback)

    ## Setup Trainer  ##
    trainer = pl.Trainer(num_sanity_val_steps=0,
                         deterministic=True,
                         accelerator='auto', devices='auto', num_nodes=1,
                         max_epochs=args.epoch_max, min_epochs=args.epoch_min,
                         precision='32',
                         logger=logger,
                         log_every_n_steps=-1,
                         callbacks=callbacks,
                         fast_dev_run=args.fast_dev_run,
                        )

    # auto-tune batch-size
    if args.autobatch:
        tuner = Tuner(trainer)
        found_batch_size = tuner.scale_batch_size(model, datamodule=datamodule,
            mode=args.autobatch, method='fit', max_trials=10, init_val=args.batch_size)
        args.batch_size_init, args.batch_size = args.batch_size, min([found_batch_size, args.autobatch_max or float('inf')])

    trainer.fit(model, datamodule=datamodule)


if __name__ == '__main__':
    parser = argparse_init()
    args = parser.parse_args()
    argparse_runtime_args(args)
    main(args)